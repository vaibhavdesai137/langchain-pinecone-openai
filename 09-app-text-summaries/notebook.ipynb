{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r ../requirements.txt -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Verify python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pinecone\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Summarizing Using Basic Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# good enough for small amount of text\n",
    "# downside: no. of tokens should be less than max supported by LLM\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "# text to summarize\n",
    "TEXT = '''\n",
    "Rust is a multi-paradigm, general-purpose programming language that emphasizes performance, type safety, and concurrency. It enforces memory safety, meaning that all references point to valid memory, without requiring the use of automated memory management techniques such as garbage collection. To simultaneously enforce memory safety and prevent data races, its \"borrow checker\" tracks the object lifetime of all references in a program during compilation. Rust was influenced by ideas from functional programming, including immutability, higher-order functions, and algebraic data types. It is popular for systems programming.[13][14][15]\n",
    "\n",
    "Software developer Graydon Hoare created Rust as a personal project while working at Mozilla Research in 2006. Mozilla officially sponsored the project in 2009. In the years following the first stable release in May 2015, Rust was adopted by companies including Amazon, Discord, Dropbox, Google (Alphabet), Meta, and Microsoft. In December 2022, it became the first language other than C and assembly to be supported in the development of the Linux kernel.\n",
    "\n",
    "Rust has been noted for its rapid adoption,[16] and has been studied in programming language theory research.\n",
    "'''\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=0)\n",
    "print(f'tokens: {llm.get_num_tokens(TEXT)}')\n",
    "messages = [\n",
    "  SystemMessage(content = 'You are an experience copywriter who is great at summarizing texts'),\n",
    "  HumanMessage(content = f'Please summarize the following text in a few words. \\n Text: {TEXT}'),\n",
    "]\n",
    "summary = llm(messages)\n",
    "print(summary.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Summarizing Using Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# good enough for small amount of text\n",
    "# but better than above because we parameterized it using prompt template\n",
    "# downside: no. of tokens should be less than max supported by LLM\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# query template\n",
    "template = '''\n",
    "- Please summarize the following text in a few words:\n",
    "- Text: {text}\n",
    "- Provide the summary in {lang} language\n",
    "'''\n",
    "\n",
    "# prompt template\n",
    "prompt = PromptTemplate(\n",
    "  input_variables = ['text', 'lang'],\n",
    "  template = template,\n",
    ")\n",
    "\n",
    "# text to summarize\n",
    "TEXT = '''\n",
    "Rust is a multi-paradigm, general-purpose programming language that emphasizes performance, type safety, and concurrency. It enforces memory safety, meaning that all references point to valid memory, without requiring the use of automated memory management techniques such as garbage collection. To simultaneously enforce memory safety and prevent data races, its \"borrow checker\" tracks the object lifetime of all references in a program during compilation. Rust was influenced by ideas from functional programming, including immutability, higher-order functions, and algebraic data types. It is popular for systems programming.[13][14][15]\n",
    "\n",
    "Software developer Graydon Hoare created Rust as a personal project while working at Mozilla Research in 2006. Mozilla officially sponsored the project in 2009. In the years following the first stable release in May 2015, Rust was adopted by companies including Amazon, Discord, Dropbox, Google (Alphabet), Meta, and Microsoft. In December 2022, it became the first language other than C and assembly to be supported in the development of the Linux kernel.\n",
    "\n",
    "Rust has been noted for its rapid adoption,[16] and has been studied in programming language theory research.\n",
    "'''\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=0)\n",
    "print(f'tokens: {llm.get_num_tokens(prompt.format(text=TEXT, lang = \"English\"))}')\n",
    "chain = LLMChain(prompt = prompt, llm = llm)\n",
    "\n",
    "summary_english = chain.run({'text': TEXT, 'lang': 'English'})\n",
    "print(f'\\nin english: {summary_english}')\n",
    "\n",
    "summary_french = chain.run({'text': TEXT, 'lang': 'French'})\n",
    "print(f'\\nin french: {summary_french}')\n",
    "\n",
    "summary_hindi = chain.run({'text': TEXT, 'lang': 'Hindi'})\n",
    "print(f'\\nin french: {summary_hindi}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Summarizing Using StuffDocumentsChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same as above two but using a StuffDocumentsChain\n",
    "# pros: makes a single call to LLM\n",
    "# cons: if the context (doc) is larger than allowed prompt size, we'll get an error\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "# query template\n",
    "template = '''\n",
    "- Please summarize the following text in a few words:\n",
    "- Text: {text}\n",
    "- Provide the summary in {lang} language\n",
    "'''\n",
    "\n",
    "# prompt template\n",
    "prompt = PromptTemplate(\n",
    "  input_variables = ['text', 'lang'],\n",
    "  template = template,\n",
    ")\n",
    "\n",
    "# init\n",
    "llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=0)\n",
    "chain = LLMChain(prompt = prompt, llm = llm)\n",
    "chain = load_summarize_chain(llm, chain_type='stuff', prompt=prompt, verbose=False)\n",
    "\n",
    "# obama's speech to summarize\n",
    "with open('files/obama-inaugural-speech.txt', encoding='utf-8') as f:\n",
    "  speech = f.read()\n",
    "docs = [Document(page_content = speech)]\n",
    "summary = chain.run({'input_documents': docs, 'lang': 'English'})\n",
    "print(f'\\nObama speech summary in english: {summary}')\n",
    "\n",
    "# trump's speech to summarize\n",
    "with open('files/trump-inaugural-speech.txt', encoding='utf-8') as f:\n",
    "  speech = f.read()\n",
    "docs = [Document(page_content = speech)]\n",
    "summary = chain.run({'input_documents': docs, 'lang': 'Spanish'})\n",
    "print(f'\\nTrump speech summary in spanish: {summary}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Summarizing Using Map-Reduce For Large Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allows summarizing very large documents\n",
    "# with map-reduce, idea is to first create chunks based on token limit\n",
    "# pass all chunks to LLM\n",
    "# it then summarizes each chunk\n",
    "# it then combines the summaries of each chunk to create the final summary\n",
    "# pros: \n",
    "# - allows summarizing very large documents\n",
    "# - each chunk summary happens in parallel\n",
    "# cons: \n",
    "# - multiple calls to LLM\n",
    "# - may lose context when summarizing individual chunks\n",
    "# - prompts are not customizable\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# obama's speech to summarize\n",
    "with open('files/obama-inaugural-speech.txt', encoding='utf-8') as f:\n",
    "  speech = f.read()\n",
    "print(f'tokens: {llm.get_num_tokens(speech)}') # 2787\n",
    "\n",
    "# using higher chunnk size to reduce number of OpenAI calls\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=5000, chunk_overlap=50)\n",
    "chunks = text_splitter.create_documents([speech])\n",
    "print(f'chunks: {len(chunks)}') # 3\n",
    "\n",
    "chain = load_summarize_chain(llm, chain_type='map_reduce', verbose=False)\n",
    "summary = chain.run(chunks)\n",
    "print(f'\\nSummary: {summary}')\n",
    "\n",
    "# internally LLM used this as 1st prompt to create summary for each chunk\n",
    "print(chain.llm_chain.prompt.template)\n",
    "# Output: \"Write a concise summary of the following: \\n\\n\\n \"{text}\" \\n\\n\\n CONCISE SUMMARY:\"\n",
    "\n",
    "# it then used this prompt to combine summaries\n",
    "print(chain.combine_document_chain.llm_chain.prompt.template)\n",
    "# Output: \"Write a concise summary of the following: \\n\\n\\n \"{text}\" \\n\\n\\n CONCISE SUMMARY:\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Summarizing Using Map-Reduce With Custom Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same as above but using prompt templates\n",
    "# this lets us customize the summary response\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# template for \"map\" phase\n",
    "map_prompt = '''\n",
    "Write a concise summary of the following text. Text: {text}\n",
    "'''\n",
    "map_prompt_template = PromptTemplate(\n",
    "  input_variables = ['text'],\n",
    "  template = map_prompt,\n",
    ")\n",
    "\n",
    "# template for \"combine\" phase\n",
    "combine_prompt = '''\n",
    "Write a concise summary of the following text in {lang} language.\n",
    "Add a title to the summary.\n",
    "The summary should have three parts.\n",
    "Part one should be INTRODUCTION.\n",
    "Part two should be a list of BULLET POINTS.\n",
    "Part three should be CONCLUSION.\n",
    "Text: {text}\n",
    "CONCISE_SUMMARY:\n",
    "'''\n",
    "combine_prompt_template = PromptTemplate(\n",
    "  input_variables = ['text', 'lang'],\n",
    "  template = combine_prompt\n",
    ")\n",
    "\n",
    "# obama's speech to summarize\n",
    "with open('files/obama-inaugural-speech.txt', encoding='utf-8') as f:\n",
    "  speech = f.read()\n",
    "print(f'tokens: {llm.get_num_tokens(speech)}') # 2787\n",
    "\n",
    "# using higher chunnk size to reduce number of OpenAI calls\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=5000, chunk_overlap=50)\n",
    "chunks = text_splitter.create_documents([speech])\n",
    "print(f'chunks: {len(chunks)}') # 3\n",
    "\n",
    "# summarize\n",
    "chain = load_summarize_chain(\n",
    "  llm, \n",
    "  chain_type='map_reduce', \n",
    "  map_prompt=map_prompt_template,\n",
    "  combine_prompt=combine_prompt_template,\n",
    "  verbose=False\n",
    ")\n",
    "summary = chain.run({'input_documents': docs, 'lang': 'Hindi'})\n",
    "print(f'\\nSummary: {summary}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Summarizing Using Refine For Large Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same as map-reduce but uses a different algorithm\n",
    "# instead of summarizing each chunk inidvidually and then combining them\n",
    "# this algorithm creates summary for 1st chunk and then uses that as context for 2nd chunk\n",
    "# and so on\n",
    "# pros:\n",
    "# - more accurate summaries\n",
    "# - less lossy than map-reduce\n",
    "# cons\n",
    "# - cannot summarize each chunk in parallel since each chunk's summary depends on summary of previous chunk\n",
    "# - takes much longer to summarize\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# obama's speech to summarize\n",
    "with open('files/obama-inaugural-speech.txt', encoding='utf-8') as f:\n",
    "  speech = f.read()\n",
    "print(f'tokens: {llm.get_num_tokens(speech)}') # 2787\n",
    "\n",
    "# using higher chunnk size to reduce number of OpenAI calls\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=5000, chunk_overlap=50)\n",
    "chunks = text_splitter.create_documents([speech])\n",
    "print(f'chunks: {len(chunks)}') # 3\n",
    "\n",
    "chain = load_summarize_chain(llm, chain_type='refine', verbose=False)\n",
    "summary = chain.run(chunks)\n",
    "print(f'\\nSummary: {summary}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Summarizing Using Refine With Custom Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same as above but uses custom prompts\n",
    "# \"existing_answer\" in the prompt below is an inbuilt langchain variable\n",
    "# changing this to something else won't work for refine template\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# template for \"first\" phase\n",
    "# remember, there is no summary to combine in this phase\n",
    "first_prompt = '''\n",
    "Write a concise summary of the following text. Text: {text}\n",
    "'''\n",
    "first_prompt_template = PromptTemplate(\n",
    "  input_variables = ['text'],\n",
    "  template = first_prompt,\n",
    ")\n",
    "\n",
    "# template for \"refine\" phase\n",
    "# summary is generated using previous chunk's summary as context plus current chunk\n",
    "refine_prompt = '''\n",
    "Write a concise summary of the following text. \n",
    "You are provided with summary upto a certain point: {existing_answer}\n",
    "In your final summary, add a title.\n",
    "The summary should have three parts.\n",
    "Part one should be INTRODUCTION.\n",
    "Part two should be a list of BULLET POINTS.\n",
    "Part three should be CONCLUSION.\n",
    "Text: {text}\n",
    "'''\n",
    "refine_prompt_template = PromptTemplate(\n",
    "  input_variables = ['existing_answer', 'text'],\n",
    "  template = refine_prompt\n",
    ")\n",
    "\n",
    "# obama's speech to summarize\n",
    "with open('files/obama-inaugural-speech.txt', encoding='utf-8') as f:\n",
    "  speech = f.read()\n",
    "print(f'tokens: {llm.get_num_tokens(speech)}') # 2787\n",
    "\n",
    "# using higher chunnk size to reduce number of OpenAI calls\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=5000, chunk_overlap=50)\n",
    "chunks = text_splitter.create_documents([speech])\n",
    "print(f'chunks: {len(chunks)}') # 3\n",
    "\n",
    "# summarize\n",
    "chain = load_summarize_chain(\n",
    "  llm, \n",
    "  chain_type='refine', \n",
    "  question_prompt=first_prompt_template,\n",
    "  refine_prompt=refine_prompt_template,\n",
    "  return_intermediate_steps=False,\n",
    "  verbose=False\n",
    ")\n",
    "summary = chain.run(chunks)\n",
    "print(f'\\nSummary: {summary}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. Summarizing Using LangChain Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use an external agent to summarize instead of using LLM's internal capabilities\n",
    "# in this example, we'll query some wikipedia page & summarize it\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.utilities import WikipediaAPIWrapper\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=0)\n",
    "wikipedia = WikipediaAPIWrapper()\n",
    "\n",
    "# configure all external tools our agent will use\n",
    "tools = [\n",
    "  Tool(\n",
    "    name='wikipedia',\n",
    "    func=wikipedia.run,\n",
    "    description='Query wikipedia for a page',\n",
    "  )\n",
    "]\n",
    "\n",
    "# initialize agent\n",
    "agent_executor = initialize_agent(\n",
    "  tools, \n",
    "  llm, \n",
    "  agent='zero-shot-react-description', \n",
    "  verbose=True\n",
    ")\n",
    "\n",
    "summary = agent_executor.run('provide a life summary for \"Barack Obama\"')\n",
    "print(f'\\nSummary: {summary}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

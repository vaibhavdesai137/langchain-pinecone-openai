{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r ../requirements.txt -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Verify python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pinecone\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Chat Bot Without Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple chatbot with no memory i.e no context between chat messages\n",
    "# each question is a new question and the model has no memory of previous questions\n",
    "# for eg: \n",
    "# __________________________________________________\n",
    "# input: 10 + 10 is\n",
    "# resp: 20\n",
    "# __________________________________________________\n",
    "# input: times 2 is\n",
    "# resp: Oh, if you multiply something by 2, you are essentially doubling it.\n",
    "# __________________________________________________\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import SystemMessage, HumanMessage, AIMessage\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "# init\n",
    "llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=1)\n",
    "prompt = ChatPromptTemplate(\n",
    "  input_variables = ['content'],\n",
    "  messages = [\n",
    "    SystemMessage(content = 'You are a chat bot having a conversation with a human'),\n",
    "    SystemMessage(content = 'Respond like how a human would and add details if applicable'),\n",
    "    HumanMessagePromptTemplate.from_template('{content}'),\n",
    "  ]\n",
    ")\n",
    "chain = LLMChain(llm = llm, prompt = prompt, verbose = False)\n",
    "\n",
    "# infinite loop\n",
    "while True:\n",
    "  \n",
    "  # get user input\n",
    "  content = input('your prompt: ')\n",
    "  \n",
    "  # break if user wants to quit\n",
    "  if content in ['quit', 'exit', 'bye', 'q']:\n",
    "    print('Goodbye!')\n",
    "    break\n",
    "  \n",
    "  # else ask the model for a response\n",
    "  resp = chain.run({'content': content})\n",
    "  print(f'input: {content}')\n",
    "  print(f'resp: {resp}')\n",
    "  print('_' * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Chat Bot With Memory (but no persistence i.e. sessions are NOT saved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple chatbot with memory i.e it saves context between chat messages\n",
    "# for eg: \n",
    "# __________________________________________________\n",
    "# input: 10 + 10 is\n",
    "# resp: 20\n",
    "# __________________________________________________\n",
    "# input: times 2 is\n",
    "# resp: 40\n",
    "# __________________________________________________\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import SystemMessage, HumanMessage, AIMessage\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# init\n",
    "llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=1)\n",
    "\n",
    "# memory_key = a key where chat messages are stored\n",
    "# return_messages = true, means that the memory will return the chat messages as a list\n",
    "memory = ConversationBufferMemory(\n",
    "  memory_key = 'chat_session', \n",
    "  return_messages = True\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate(\n",
    "  input_variables = ['content'],\n",
    "  messages = [\n",
    "    SystemMessage(content = 'You are a chat bot having a conversation with a human'),\n",
    "    SystemMessage(content = 'Respond like how a human would and add details if applicable'),\n",
    "    MessagesPlaceholder(variable_name = 'chat_session'), # where the memory will be stored\n",
    "    HumanMessagePromptTemplate.from_template('{content}'),\n",
    "  ]\n",
    ")\n",
    "chain = LLMChain(\n",
    "  llm = llm, \n",
    "  prompt = prompt, \n",
    "  memory = memory, # add memory to chain\n",
    "  verbose = False)\n",
    "\n",
    "# infinite loop\n",
    "while True:\n",
    "  \n",
    "  # get user input\n",
    "  content = input('your prompt: ')\n",
    "  \n",
    "  # break if user wants to quit\n",
    "  if content in ['quit', 'exit', 'bye', 'q']:\n",
    "    print('Goodbye!')\n",
    "    break\n",
    "  \n",
    "  # else ask the model for a response\n",
    "  resp = chain.run({'content': content})\n",
    "  print(f'input: {content}')\n",
    "  print(f'resp: {resp}')\n",
    "  print('_' * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Chat Bot With Memory & Saved Sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple chatbot with memory & persistence\n",
    "# for eg: \n",
    "# __________________________________________________\n",
    "# input: 10 + 10 is\n",
    "# resp: 20\n",
    "# __________________________________________________\n",
    "# input: times 2 is\n",
    "# resp: 40\n",
    "# __________________________________________________\n",
    "# quit and re-run program\n",
    "# __________________________________________________\n",
    "# input: plus 10 is\n",
    "# resp: 50\n",
    "# __________________________________________________\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import SystemMessage, HumanMessage, AIMessage\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder\n",
    "from langchain.memory import ConversationBufferMemory, FileChatMessageHistory\n",
    "\n",
    "# init\n",
    "llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=1)\n",
    "\n",
    "# save chat history to a file\n",
    "history = FileChatMessageHistory('chat_history.json')\n",
    "\n",
    "# chat_memory = tell llm to use the chat history. if you run the program again, it will remember prev questions and answer accordingly\n",
    "# i.e. we now have persistent memory\n",
    "memory = ConversationBufferMemory(\n",
    "  memory_key = 'chat_session', \n",
    "  chat_memory = history, # add chat history to memory\n",
    "  return_messages = True\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate(\n",
    "  input_variables = ['content'],\n",
    "  messages = [\n",
    "    SystemMessage(content = 'You are a chat bot having a conversation with a human'),\n",
    "    SystemMessage(content = 'Respond like how a human would and add details if applicable'),\n",
    "    MessagesPlaceholder(variable_name = 'chat_session'),\n",
    "    HumanMessagePromptTemplate.from_template('{content}'),\n",
    "  ]\n",
    ")\n",
    "chain = LLMChain(\n",
    "  llm = llm, \n",
    "  prompt = prompt, \n",
    "  memory = memory,\n",
    "  verbose = False)\n",
    "\n",
    "# infinite loop\n",
    "while True:\n",
    "  \n",
    "  # get user input\n",
    "  content = input('your prompt: ')\n",
    "  \n",
    "  # break if user wants to quit\n",
    "  if content in ['quit', 'exit', 'bye', 'q']:\n",
    "    print('Goodbye!')\n",
    "    break\n",
    "  \n",
    "  # else ask the model for a response\n",
    "  resp = chain.run({'content': content})\n",
    "  print(f'input: {content}')\n",
    "  print(f'resp: {resp}')\n",
    "  print('_' * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r ../requirements.txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip show langchain openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Verify python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)\n",
    "\n",
    "os.environ.get(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. LLM Text Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init text based llm\n",
    "from langchain import OpenAI\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0.7, max_tokens=512)\n",
    "print(llm)\n",
    "\n",
    "# get no. of tokens\n",
    "print('\\n\\nnum tokens: ' + str(llm.get_num_tokens('tell me a joke')))  \n",
    "  \n",
    "# get output for one question\n",
    "output1 = llm('explain java in one sentence')\n",
    "print('\\n\\noutput1: ' + output1)\n",
    "\n",
    "# get output for multiple questions\n",
    "output2 = llm.generate(['what is the capital of India', 'explain c# in one sentence'])\n",
    "print('no. of answers: ' + str(len(output2.generations)))\n",
    "print(output2)\n",
    "print('\\n1st answer: ' + output2.generations[0][0].text)\n",
    "print('\\n2nd answer: ' + output2.generations[1][0].text)\n",
    "\n",
    "# get multiple answers for a single question\n",
    "output3 = llm.generate(['Tell me a one liner joke'] * 3)\n",
    "for o in output3.generations:\n",
    "  print('\\n\\njoke: ' + o[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. LLM Chat Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init chat based llm\n",
    "from langchain.schema import (AIMessage, SystemMessage, HumanMessage)\n",
    "from langchain.chat_models import  ChatOpenAI\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.5, max_tokens=1024)\n",
    "print(llm)\n",
    "\n",
    "# start chatting\n",
    "messages = [\n",
    "  # set context for our assistant\n",
    "  SystemMessage(content='You are doctor in India and respond only in Hindi'),\n",
    "  # ask a question as a user\n",
    "  HumanMessage(content='I have fever. What off the shelf medicine should I take? Answer iun one line please'),\n",
    "]\n",
    "output = llm(messages)\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain import OpenAI\n",
    "\n",
    "template = '''\n",
    "You are an experienced doctor. Write a one line answer for {disease} in {language}\n",
    "'''\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "  input_variables=['disease', 'language'], \n",
    "  template=template\n",
    ")\n",
    "print(prompt)\n",
    "\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0.7, max_tokens=512)\n",
    "print(llm)\n",
    "\n",
    "output = llm(prompt.format(disease='diabetes', language='english'))\n",
    "print(output)\n",
    "output = llm(prompt.format(disease='jaundice', language='hindi'))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Simple Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# create prompt\n",
    "template = '''\n",
    "You are an experienced doctor. Write a one line answer for {disease} in {language}\n",
    "'''\n",
    "prompt = PromptTemplate(\n",
    "  input_variables=['disease', 'language'], \n",
    "  template=template\n",
    ")\n",
    "\n",
    "# create llm\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0.7, max_tokens=512)\n",
    "\n",
    "#  create chain\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "#  run chain\n",
    "output = chain.run({'disease': 'diabetes', 'language': 'english'})\n",
    "print(output)\n",
    "output = chain.run({'disease': 'jaundice', 'language': 'hindi'})\n",
    "print(output)\n",
    "output = chain.run({'disease': 'fever', 'language': 'german'})\n",
    "print(output)\n",
    "\n",
    "# if the prompt had just one variable, we could simply do this\n",
    "# output = chain.run('fever')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Sequential Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "\n",
    "# create llm 1\n",
    "llm_1 = OpenAI(model_name=\"text-davinci-003\", temperature=0.7, max_tokens=512)\n",
    "prompt_1 = PromptTemplate(\n",
    "  input_variables=['concept'], \n",
    "  template='''You are an experienced python programmer. Write a simple easy to understand function that implements the concept of {concept}'''\n",
    ")\n",
    "chain_1 = LLMChain(llm=llm_1, prompt=prompt_1)\n",
    "\n",
    "# create llm 2\n",
    "llm_2 = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.7, max_tokens=512)\n",
    "prompt_2 = PromptTemplate(\n",
    "  input_variables=['function'], \n",
    "  template='''Given a python function {function}, describe it in detail'''\n",
    ")\n",
    "chain_2 = LLMChain(llm=llm_2, prompt=prompt_2)\n",
    "\n",
    "# create sequential chain\n",
    "chain = SimpleSequentialChain(chains=[chain_1, chain_2], verbose=True)\n",
    "\n",
    "# run chain\n",
    "output = chain.run('recursion')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. LangChain Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.agents.agent_toolkits import create_python_agent\n",
    "from langchain_experimental.tools.python.tool import PythonREPLTool\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "agent_executor = create_python_agent(\n",
    "  llm=llm, \n",
    "  tool=PythonREPLTool(description='put the final command inside print()'),\n",
    "  verbose=True,\n",
    ")\n",
    "agent_executor.run('calculate the square root of factorial of 20 and display it in 3 decimal points')\n",
    "agent_executor.run('what is the answer to 5.1 ** 7.3?')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

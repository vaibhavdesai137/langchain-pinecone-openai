{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r ../requirements.txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: langchain\n",
      "Version: 0.0.352\n",
      "Summary: Building applications with LLMs through composability\n",
      "Home-page: https://github.com/langchain-ai/langchain\n",
      "Author: \n",
      "Author-email: \n",
      "License: MIT\n",
      "Location: /Users/vdesai/Dev/git/vaibhavdesai137/langchain-pinecone-openai/.venv/lib/python3.11/site-packages\n",
      "Requires: aiohttp, dataclasses-json, jsonpatch, langchain-community, langchain-core, langsmith, numpy, pydantic, PyYAML, requests, SQLAlchemy, tenacity\n",
      "Required-by: langchain-experimental\n",
      "---\n",
      "Name: openai\n",
      "Version: 0.28.1\n",
      "Summary: Python client library for the OpenAI API\n",
      "Home-page: https://github.com/openai/openai-python\n",
      "Author: OpenAI\n",
      "Author-email: support@openai.com\n",
      "License: \n",
      "Location: /Users/vdesai/Dev/git/vaibhavdesai137/langchain-pinecone-openai/.venv/lib/python3.11/site-packages\n",
      "Requires: aiohttp, requests, tqdm\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show langchain openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Verify python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'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'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)\n",
    "\n",
    "os.environ.get(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. LLM Text Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mOpenAI\u001b[0m\n",
      "Params: {'model_name': 'gpt-3.5-turbo-instruct', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 512}\n",
      "\n",
      "\n",
      "num tokens: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "output1: \n",
      "\n",
      "Java is an object-oriented programming language that is widely used for building desktop, web, and mobile applications.\n",
      "no. of answers: 2\n",
      "generations=[[Generation(text='\\n\\nThe capital of India is New Delhi.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nC# is an object-oriented programming language designed by Microsoft for building a variety of applications on the .NET framework.', generation_info={'finish_reason': 'stop', 'logprobs': None})]] llm_output={'token_usage': {'prompt_tokens': 12, 'completion_tokens': 33, 'total_tokens': 45}, 'model_name': 'gpt-3.5-turbo-instruct'} run=[RunInfo(run_id=UUID('d09ee87f-7f07-40d0-a4c1-5ab4e2c170eb')), RunInfo(run_id=UUID('7ea71beb-5080-4ed7-b540-2c7277fb9f72'))]\n",
      "\n",
      "1st answer: \n",
      "\n",
      "The capital of India is New Delhi.\n",
      "\n",
      "2nd answer: \n",
      "\n",
      "C# is an object-oriented programming language designed by Microsoft for building a variety of applications on the .NET framework.\n",
      "\n",
      "\n",
      "joke: \n",
      "\n",
      "Why couldn't the bicycle stand up by itself? Because it was two-tired.\n",
      "\n",
      "\n",
      "joke: \n",
      "\n",
      "Why don't scientists trust atoms?\n",
      "\n",
      "Because they make up everything!\n",
      "\n",
      "\n",
      "joke: \n",
      "\n",
      "Why don't scientists trust atoms? Because they make up everything.\n"
     ]
    }
   ],
   "source": [
    "# init text based llm\n",
    "from langchain import OpenAI\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0.7, max_tokens=512)\n",
    "print(llm)\n",
    "\n",
    "# get no. of tokens\n",
    "print('\\n\\nnum tokens: ' + str(llm.get_num_tokens('tell me a joke')))  \n",
    "  \n",
    "# get output for one question\n",
    "output1 = llm('explain java in one sentence')\n",
    "print('\\n\\noutput1: ' + output1)\n",
    "\n",
    "# get output for multiple questions\n",
    "output2 = llm.generate(['what is the capital of India', 'explain c# in one sentence'])\n",
    "print('no. of answers: ' + str(len(output2.generations)))\n",
    "print(output2)\n",
    "print('\\n1st answer: ' + output2.generations[0][0].text)\n",
    "print('\\n2nd answer: ' + output2.generations[1][0].text)\n",
    "\n",
    "# get multiple answers for a single question\n",
    "output3 = llm.generate(['Tell me a one liner joke'] * 3)\n",
    "for o in output3.generations:\n",
    "  print('\\n\\njoke: ' + o[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. LLM Chat Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<class 'openai.api_resources.chat_completion.ChatCompletion'> temperature=0.5 openai_api_key='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' openai_proxy='' max_tokens=1024\n",
      "आप परासिटामोल ले सकते हैं।\n"
     ]
    }
   ],
   "source": [
    "# init chat based llm\n",
    "from langchain.schema import (AIMessage, SystemMessage, HumanMessage)\n",
    "from langchain.chat_models import  ChatOpenAI\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.5, max_tokens=1024)\n",
    "print(llm)\n",
    "\n",
    "# start chatting\n",
    "messages = [\n",
    "  # set context for our assistant\n",
    "  SystemMessage(content='You are doctor in India and respond only in Hindi'),\n",
    "  # ask a question as a user\n",
    "  HumanMessage(content='I have fever. What off the shelf medicine should I take? Answer iun one line please'),\n",
    "]\n",
    "output = llm(messages)\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['disease', 'language'] template='\\nYou are an experienced doctor. Write a one line answer for {disease} in {language}\\n'\n",
      "\u001b[1mOpenAI\u001b[0m\n",
      "Params: {'model_name': 'gpt-3.5-turbo-instruct', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 512}\n",
      "\n",
      "Diabetes is a chronic condition characterized by high levels of sugar in the blood.\n",
      "पीलिया हिंदी में हैं।\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain import OpenAI\n",
    "\n",
    "template = '''\n",
    "You are an experienced doctor. Write a one line answer for {disease} in {language}\n",
    "'''\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "  input_variables=['disease', 'language'], \n",
    "  template=template\n",
    ")\n",
    "print(prompt)\n",
    "\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0.7, max_tokens=512)\n",
    "print(llm)\n",
    "\n",
    "output = llm(prompt.format(disease='diabetes', language='english'))\n",
    "print(output)\n",
    "output = llm(prompt.format(disease='jaundice', language='hindi'))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Simple Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Diabetes is a chronic condition in which the body is unable to properly regulate blood sugar levels.\n",
      "\n",
      "जॉन्डिस- पीलिया रोग की एक स्थिति है जिसमें त्वचा और आंखों का पीलापन होता है।\n",
      "\n",
      "Das ist Fieber.\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# create prompt\n",
    "template = '''\n",
    "You are an experienced doctor. Write a one line answer for {disease} in {language}\n",
    "'''\n",
    "prompt = PromptTemplate(\n",
    "  input_variables=['disease', 'language'], \n",
    "  template=template\n",
    ")\n",
    "\n",
    "# create llm\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0.7, max_tokens=512)\n",
    "\n",
    "#  create chain\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "#  run chain\n",
    "output = chain.run({'disease': 'diabetes', 'language': 'english'})\n",
    "print(output)\n",
    "output = chain.run({'disease': 'jaundice', 'language': 'hindi'})\n",
    "print(output)\n",
    "output = chain.run({'disease': 'fever', 'language': 'german'})\n",
    "print(output)\n",
    "\n",
    "# if the prompt had just one variable, we could simply do this\n",
    "# output = chain.run('fever')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Sequential Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m\n",
      "\n",
      "Recursion is a programming technique where a function calls itself until a specific condition is met. It is useful for solving problems that can be broken down into smaller and simpler subproblems.\n",
      "\n",
      "Here is a simple function that calculates the factorial of a number using recursion:\n",
      "\n",
      "def factorial(n):\n",
      "    # base case: if n is 0 or 1, return 1\n",
      "    if n == 0 or n == 1:\n",
      "        return 1\n",
      "    # recursive case: call the function again with n-1\n",
      "    else:\n",
      "        return n * factorial(n-1)\n",
      "\n",
      "# example:\n",
      "print(factorial(5)) # output: 120 (5 * 4 * 3 * 2 * 1)\n",
      "\n",
      "In this function, we first check for the base case where n is equal to 0 or 1. If it is, we return 1 as the factorial of 0 and 1 is 1. If not, we call the function again with n-1 until the base case is reached. This process continues until n becomes 0 or 1, and then the function starts returning the calculated values in reverse order.\n",
      "\n",
      "Recursion can be a powerful tool, but it is important to ensure that the base case is reachable and that the function will eventually terminate. Otherwise, it can lead to an infinite loop. \u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mThe given python function calculates the factorial of a number using recursion. \n",
      "\n",
      "Factorial is the product of all positive integers from 1 to the given number. For example, the factorial of 5 is calculated as 5 * 4 * 3 * 2 * 1 = 120.\n",
      "\n",
      "The function takes an input parameter 'n' which represents the number for which the factorial needs to be calculated.\n",
      "\n",
      "The function first checks for the base case, which is when n is equal to 0 or 1. If the base case is true, the function immediately returns 1 as the factorial of 0 and 1 is 1.\n",
      "\n",
      "If the base case is not true, the function enters the recursive case. In the recursive case, the function calls itself with the argument n-1. This means that the function calculates the factorial of n-1 and multiplies it with n. This process continues recursively until the base case is reached.\n",
      "\n",
      "Once the base case is reached (n becomes 0 or 1), the function starts returning the calculated values in reverse order. Each recursive call returns the product of n and the factorial of n-1, which in turn returns the product of (n-1) and the factorial of (n-2), and so on, until the initial call to the function returns the final factorial value.\n",
      "\n",
      "In the example given, the function factorial(5) is called and it returns the value 5 * 4 * 3 * 2 * 1 = 120, which is printed as the output.\n",
      "\n",
      "It is important to note that recursion should be used with caution as it can lead to infinite loops if the base case is not reachable or if the termination condition is not handled properly.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "\n",
    "# create llm 1\n",
    "llm_1 = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0.7, max_tokens=512)\n",
    "prompt_1 = PromptTemplate(\n",
    "  input_variables=['concept'], \n",
    "  template='''You are an experienced python programmer. Write a simple easy to understand function that implements the concept of {concept}'''\n",
    ")\n",
    "chain_1 = LLMChain(llm=llm_1, prompt=prompt_1)\n",
    "\n",
    "# create llm 2\n",
    "llm_2 = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.7, max_tokens=512)\n",
    "prompt_2 = PromptTemplate(\n",
    "  input_variables=['function'], \n",
    "  template='''Given a python function {function}, describe it in detail'''\n",
    ")\n",
    "chain_2 = LLMChain(llm=llm_2, prompt=prompt_2)\n",
    "\n",
    "# create sequential chain\n",
    "chain = SimpleSequentialChain(chains=[chain_1, chain_2], verbose=True)\n",
    "\n",
    "# run chain\n",
    "output = chain.run('recursion')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. LangChain Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python REPL can execute arbitrary code. Use with caution.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m I can use the math library to calculate the square root and factorial of 20\n",
      "Action: Python_REPL\n",
      "Action Input: import math\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I can use the math library to calculate the square root and factorial of 20\n",
      "Action: Python_REPL\n",
      "Action Input: math.sqrt(math.factorial(20))\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I can use the round function to display the result in 3 decimal points\n",
      "Action: Python_REPL\n",
      "Action Input: round(math.sqrt(math.factorial(20)), 3)\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer:  25852016738884976640000.000\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I can use the power operator to raise 5.1 to the power of 7.3\n",
      "Action: Python_REPL\n",
      "Action Input: print(5.1 ** 7.3)\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m146306.05007233328\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: 146306.05007233328\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'146306.05007233328'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_experimental.agents.agent_toolkits import create_python_agent\n",
    "from langchain_experimental.tools.python.tool import PythonREPLTool\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "agent_executor = create_python_agent(\n",
    "  llm=llm, \n",
    "  tool=PythonREPLTool(description='put the final command inside print()'),\n",
    "  verbose=True,\n",
    ")\n",
    "agent_executor.run('calculate the square root of factorial of 20 and display it in 3 decimal points')\n",
    "agent_executor.run('what is the answer to 5.1 ** 7.3?')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
